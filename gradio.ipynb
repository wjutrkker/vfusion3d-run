{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone  https://github.com/facebookresearch/vfusion3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "import torch\n",
    "import gradio as gr\n",
    "import os\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import mcubes\n",
    "import imageio\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from rembg import remove, new_session\n",
    "from functools import partial\n",
    "from kiui.op import recenter\n",
    "import kiui\n",
    "from gradio_litmodel3d import LitModel3D\n",
    "\n",
    "# we load the pre-trained model from HF\n",
    "class LRMGeneratorWrapper:\n",
    "    def __init__(self):\n",
    "        self.config = AutoConfig.from_pretrained(\"jadechoghari/vfusion3d\", trust_remote_code=True)\n",
    "        self.model = AutoModel.from_pretrained(\"jadechoghari/vfusion3d\", trust_remote_code=True)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def forward(self, image, camera):\n",
    "        return self.model(image, camera)\n",
    "\n",
    "model_wrapper = LRMGeneratorWrapper()\n",
    "\n",
    "# we preprocess the input image\n",
    "def preprocess_image(image, source_size):\n",
    "    session = new_session(\"isnet-general-use\")\n",
    "    rembg_remove = partial(remove, session=session)\n",
    "    image = np.array(image)\n",
    "    image = rembg_remove(image)\n",
    "    mask = rembg_remove(image, only_mask=True)\n",
    "    image = recenter(image, mask, border_ratio=0.20)\n",
    "    image = torch.tensor(image).permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "    if image.shape[1] == 4:\n",
    "        image = image[:, :3, ...] * image[:, 3:, ...] + (1 - image[:, 3:, ...])\n",
    "    image = torch.nn.functional.interpolate(image, size=(source_size, source_size), mode='bicubic', align_corners=True)\n",
    "    image = torch.clamp(image, 0, 1)\n",
    "    return image\n",
    "\n",
    "# Copied from https://github.com/facebookresearch/vfusion3d/blob/main/lrm/cam_utils.py and\n",
    "# https://github.com/facebookresearch/vfusion3d/blob/main/lrm/inferrer.py\n",
    "def get_normalized_camera_intrinsics(intrinsics: torch.Tensor):\n",
    "    fx, fy = intrinsics[:, 0, 0], intrinsics[:, 0, 1]\n",
    "    cx, cy = intrinsics[:, 1, 0], intrinsics[:, 1, 1]\n",
    "    width, height = intrinsics[:, 2, 0], intrinsics[:, 2, 1]\n",
    "    fx, fy = fx / width, fy / height\n",
    "    cx, cy = cx / width, cy / height\n",
    "    return fx, fy, cx, cy\n",
    "\n",
    "def build_camera_principle(RT: torch.Tensor, intrinsics: torch.Tensor):\n",
    "    fx, fy, cx, cy = get_normalized_camera_intrinsics(intrinsics)\n",
    "    return torch.cat([\n",
    "        RT.reshape(-1, 12),\n",
    "        fx.unsqueeze(-1), fy.unsqueeze(-1), cx.unsqueeze(-1), cy.unsqueeze(-1),\n",
    "    ], dim=-1)\n",
    "\n",
    "def _default_intrinsics():\n",
    "    fx = fy = 384\n",
    "    cx = cy = 256\n",
    "    w = h = 512\n",
    "    intrinsics = torch.tensor([\n",
    "        [fx, fy],\n",
    "        [cx, cy],\n",
    "        [w, h],\n",
    "    ], dtype=torch.float32)\n",
    "    return intrinsics\n",
    "\n",
    "def _default_source_camera(batch_size: int = 1):\n",
    "    canonical_camera_extrinsics = torch.tensor([[\n",
    "        [0, 0, 1, 1],\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 1, 0, 0],\n",
    "    ]], dtype=torch.float32)\n",
    "    canonical_camera_intrinsics = _default_intrinsics().unsqueeze(0)\n",
    "    source_camera = build_camera_principle(canonical_camera_extrinsics, canonical_camera_intrinsics)\n",
    "    return source_camera.repeat(batch_size, 1)\n",
    "\n",
    "def _center_looking_at_camera_pose(camera_position: torch.Tensor, look_at: torch.Tensor = None, up_world: torch.Tensor = None):\n",
    "    \"\"\"\n",
    "    camera_position: (M, 3)\n",
    "    look_at: (3)\n",
    "    up_world: (3)\n",
    "    return: (M, 3, 4)\n",
    "    \"\"\"\n",
    "    # by default, looking at the origin and world up is pos-z\n",
    "    if look_at is None:\n",
    "        look_at = torch.tensor([0, 0, 0], dtype=torch.float32)\n",
    "    if up_world is None:\n",
    "        up_world = torch.tensor([0, 0, 1], dtype=torch.float32)\n",
    "    look_at = look_at.unsqueeze(0).repeat(camera_position.shape[0], 1)\n",
    "    up_world = up_world.unsqueeze(0).repeat(camera_position.shape[0], 1)\n",
    "\n",
    "    z_axis = camera_position - look_at\n",
    "    z_axis = z_axis / z_axis.norm(dim=-1, keepdim=True)\n",
    "    x_axis = torch.cross(up_world, z_axis)\n",
    "    x_axis = x_axis / x_axis.norm(dim=-1, keepdim=True)\n",
    "    y_axis = torch.cross(z_axis, x_axis)\n",
    "    y_axis = y_axis / y_axis.norm(dim=-1, keepdim=True)\n",
    "    extrinsics = torch.stack([x_axis, y_axis, z_axis, camera_position], dim=-1)\n",
    "    return extrinsics\n",
    "\n",
    "def compose_extrinsic_RT(RT: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compose the standard form extrinsic matrix from RT.\n",
    "    Batched I/O.\n",
    "    \"\"\"\n",
    "    return torch.cat([\n",
    "        RT,\n",
    "        torch.tensor([[[0, 0, 0, 1]]], dtype=torch.float32).repeat(RT.shape[0], 1, 1).to(RT.device)\n",
    "        ], dim=1)\n",
    "\n",
    "def _build_camera_standard(RT: torch.Tensor, intrinsics: torch.Tensor):\n",
    "    \"\"\"\n",
    "    RT: (N, 3, 4)\n",
    "    intrinsics: (N, 3, 2), [[fx, fy], [cx, cy], [width, height]]\n",
    "    \"\"\"\n",
    "    E = compose_extrinsic_RT(RT)\n",
    "    fx, fy, cx, cy = get_normalized_camera_intrinsics(intrinsics)\n",
    "    I = torch.stack([\n",
    "        torch.stack([fx, torch.zeros_like(fx), cx], dim=-1),\n",
    "        torch.stack([torch.zeros_like(fy), fy, cy], dim=-1),\n",
    "        torch.tensor([[0, 0, 1]], dtype=torch.float32, device=RT.device).repeat(RT.shape[0], 1),\n",
    "    ], dim=1)\n",
    "    return torch.cat([\n",
    "        E.reshape(-1, 16),\n",
    "        I.reshape(-1, 9),\n",
    "    ], dim=-1)\n",
    "\n",
    "def _default_render_cameras(batch_size: int = 1):\n",
    "    M = 80\n",
    "    radius = 1.5\n",
    "    elevation = 0\n",
    "    camera_positions = []\n",
    "    rand_theta = np.random.uniform(0, np.pi/180)\n",
    "    elevation = np.radians(elevation)\n",
    "    for i in range(M):\n",
    "        theta = 2 * np.pi * i / M + rand_theta\n",
    "        x = radius * np.cos(theta) * np.cos(elevation)\n",
    "        y = radius * np.sin(theta) * np.cos(elevation)\n",
    "        z = radius * np.sin(elevation)\n",
    "        camera_positions.append([x, y, z])\n",
    "    camera_positions = torch.tensor(camera_positions, dtype=torch.float32)\n",
    "    extrinsics = _center_looking_at_camera_pose(camera_positions)\n",
    "\n",
    "    render_camera_intrinsics = _default_intrinsics().unsqueeze(0).repeat(extrinsics.shape[0], 1, 1)\n",
    "    render_cameras = _build_camera_standard(extrinsics, render_camera_intrinsics)\n",
    "    return render_cameras.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "def generate_mesh(image, source_size=512, render_size=384, mesh_size=512, export_mesh=False, export_video=True, fps=30):\n",
    "    image = preprocess_image(image, source_size).to(model_wrapper.device)\n",
    "    source_camera = _default_source_camera(batch_size=1).to(model_wrapper.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        planes = model_wrapper.forward(image, source_camera)\n",
    "\n",
    "        if export_mesh:\n",
    "            grid_out = model_wrapper.model.synthesizer.forward_grid(planes=planes, grid_size=mesh_size)\n",
    "            vtx, faces = mcubes.marching_cubes(grid_out['sigma'].float().squeeze(0).squeeze(-1).cpu().numpy(), 1.0)\n",
    "            vtx = vtx / (mesh_size - 1) * 2 - 1\n",
    "            vtx_tensor = torch.tensor(vtx, dtype=torch.float32, device=model_wrapper.device).unsqueeze(0)\n",
    "            vtx_colors = model_wrapper.model.synthesizer.forward_points(planes, vtx_tensor)['rgb'].float().squeeze(0).cpu().numpy()\n",
    "            vtx_colors = (vtx_colors * 255).astype(np.uint8)\n",
    "            mesh = trimesh.Trimesh(vertices=vtx, faces=faces, vertex_colors=vtx_colors)\n",
    "\n",
    "            mesh_path = \"awesome_mesh.obj\"\n",
    "            mesh.export(mesh_path, 'obj')\n",
    "\n",
    "            return mesh_path, mesh_path\n",
    "\n",
    "        if export_video:\n",
    "            render_cameras = _default_render_cameras(batch_size=1).to(model_wrapper.device)\n",
    "            frames = []\n",
    "            chunk_size = 1\n",
    "            for i in range(0, render_cameras.shape[1], chunk_size):\n",
    "                frame_chunk = model_wrapper.model.synthesizer(\n",
    "                    planes,\n",
    "                    render_cameras[:, i:i + chunk_size],\n",
    "                    render_size,\n",
    "                    render_size,\n",
    "                    0,\n",
    "                    0\n",
    "                )\n",
    "                frames.append(frame_chunk['images_rgb'])\n",
    "\n",
    "            frames = torch.cat(frames, dim=1)\n",
    "            frames = frames.squeeze(0)\n",
    "            frames = (frames.permute(0, 2, 3, 1).cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "            video_path = \"awesome_video.mp4\"\n",
    "            imageio.mimwrite(video_path, frames, fps=fps)\n",
    "\n",
    "            return None, video_path\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def step_1_generate_obj(image):\n",
    "    mesh_path, _ = generate_mesh(image, export_mesh=True)\n",
    "    return mesh_path, mesh_path\n",
    "\n",
    "def step_2_generate_video(image):\n",
    "    _, video_path = generate_mesh(image, export_video=True)\n",
    "    return video_path\n",
    "\n",
    "def step_3_display_3d_model(mesh_file):\n",
    "    return mesh_file\n",
    "\n",
    "# set up the example files from assets folder, we limit to 10\n",
    "example_folder = \"assets\"\n",
    "examples = [os.path.join(example_folder, f) for f in os.listdir(example_folder) if f.endswith(('.png', '.jpg', '.jpeg'))][:10]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        \n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"\"\"\n",
    "            # Welcome to [VFusion3D](https://junlinhan.github.io/projects/vfusion3d.html) Demo\n",
    "\n",
    "            This demo allows you to upload an image and generate a 3D model or rendered videos from it. \n",
    "\n",
    "            ## How to Use:\n",
    "            1. Click on \"Click to Upload\" to upload an image, or choose one example image.\n",
    "            \n",
    "            2: Choose between \"Generate and Download Mesh\" or \"Generate and Download Video\", then click it.\n",
    "            \n",
    "            3. Wait for the model to process; meshes should take approximately 10 seconds, and videos will take approximately 30 seconds.\n",
    "            \n",
    "            4. Download the generated mesh or video.\n",
    "\n",
    "            This demo does not aim to provide optimal results but rather to provide a quick look. See our [GitHub](https://github.com/facebookresearch/vfusion3d) for more. \n",
    "\n",
    "            \"\"\")\n",
    "            img_input = gr.Image(type=\"pil\", label=\"Input Image\")\n",
    "            examples_component = gr.Examples(examples=examples, inputs=img_input, outputs=None, examples_per_page=3)\n",
    "            generate_mesh_button = gr.Button(\"Generate and Download Mesh\")\n",
    "            generate_video_button = gr.Button(\"Generate and Download Video\")\n",
    "            obj_file_output = gr.File(label=\"Download .obj File\")\n",
    "            video_file_output = gr.File(label=\"Download Video\")\n",
    "\n",
    "        with gr.Column():\n",
    "            model_output = LitModel3D(\n",
    "                clear_color=[0.1, 0.1, 0.1, 0],  # can adjust background color for better contrast\n",
    "                label=\"3D Model Visualization\",\n",
    "                scale=1.0,\n",
    "                tonemapping=\"aces\",  # can use aces tonemapping for more realistic lighting\n",
    "                exposure=1.0,        # can adjust exposure to control brightness\n",
    "                contrast=1.1,        # can slightly increase contrast for better depth\n",
    "                camera_position=(0, 0, 2),  # will set initial camera position to center the model\n",
    "                zoom_speed=0.5,      # will adjust zoom speed for better control\n",
    "                pan_speed=0.5,       # will adjust pan speed for better control\n",
    "                interactive=True     # this allow users to interact with the model\n",
    "            )\n",
    "            \n",
    "        \n",
    "    # clear outputs\n",
    "    def clear_model_viewer():\n",
    "        \"\"\"Reset the Model3D component before loading a new model.\"\"\"\n",
    "        return gr.update(value=None)\n",
    "    \n",
    "    def generate_and_visualize(image):\n",
    "        mesh_path = step_1_generate_obj(image)\n",
    "        return mesh_path, mesh_path\n",
    "\n",
    "    # first we clear the existing 3D model\n",
    "    img_input.change(clear_model_viewer, inputs=None, outputs=model_output)\n",
    "\n",
    "    # then, generate the mesh and video\n",
    "    generate_mesh_button.click(step_1_generate_obj, inputs=img_input, outputs=[obj_file_output, model_output])\n",
    "    generate_video_button.click(step_2_generate_video, inputs=img_input, outputs=video_file_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
